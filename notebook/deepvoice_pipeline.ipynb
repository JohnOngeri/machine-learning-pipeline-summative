{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deepfake Voice Detection Pipeline\n",
    "\n",
    "This notebook demonstrates the complete machine learning pipeline for detecting deepfake voices.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading and Exploration](#data-loading)\n",
    "2. [Feature Analysis](#feature-analysis)\n",
    "3. [Model Training](#model-training)\n",
    "4. [Model Evaluation](#model-evaluation)\n",
    "5. [Feature Importance](#feature-importance)\n",
    "6. [Model Deployment](#model-deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration {#data-loading}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "url = \"https://hebbkx1anhila5yf.public.blob.vercel-storage.com/DATASET-balanced-JcqFJYhgnWK5P8zrmIuuMwyj9BIpH9.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['LABEL'].value_counts())\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"Dataset Info:\")\n",
    "print(f\"Total samples: {len(df)}\")\n",
    "print(f\"Features: {len(df.columns) - 1}\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label distribution visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Count plot\n",
    "df['LABEL'].value_counts().plot(kind='bar', ax=ax1, color=['#FF6B6B', '#4ECDC4'])\n",
    "ax1.set_title('Label Distribution')\n",
    "ax1.set_xlabel('Label')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Pie chart\n",
    "df['LABEL'].value_counts().plot(kind='pie', ax=ax2, autopct='%1.1f%%', colors=['#FF6B6B', '#4ECDC4'])\n",
    "ax2.set_title('Label Distribution (Percentage)')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Analysis {#feature-analysis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature columns\n",
    "feature_columns = [col for col in df.columns if col != 'LABEL']\n",
    "print(f\"Feature columns ({len(feature_columns)}): {feature_columns}\")\n",
    "\n",
    "# Convert string columns to numeric\n",
    "for col in feature_columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(f\"\\nMissing values after conversion: {df[feature_columns].isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "df[feature_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(15, 12))\n",
    "correlation_matrix = df[feature_columns].corr()\n",
    "sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.1)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by label\n",
    "# Select a few key features for visualization\n",
    "key_features = ['chroma_stft', 'rms', 'spectral_centroid', 'spectral_bandwidth', \n",
    "                'rolloff', 'zero_crossing_rate', 'mfcc1', 'mfcc2']\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    for label in ['REAL', 'FAKE']:\n",
    "        data = df[df['LABEL'] == label][feature]\n",
    "        axes[i].hist(data, alpha=0.7, label=label, bins=30)\n",
    "    \n",
    "    axes[i].set_title(f'{feature} Distribution')\n",
    "    axes[i].set_xlabel(feature)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for key features\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, feature in enumerate(key_features):\n",
    "    sns.boxplot(data=df, x='LABEL', y=feature, ax=axes[i])\n",
    "    axes[i].set_title(f'{feature} by Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training {#model-training}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X = df[feature_columns].values\n",
    "y = df['LABEL'].values\n",
    "\n",
    "# Handle any remaining NaN values\n",
    "X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Label distribution: {np.bincount(y_encoded)}\")\n",
    "print(f\"Label mapping: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"Training label distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test label distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "model_results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    model_results[name] = {\n",
    "        'model': model,\n",
    "        'cv_scores': cv_scores,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'y_pred': y_pred,\n",
    "        'y_pred_proba': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"CV F1 Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation {#model-evaluation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(model_results.keys()),\n",
    "    'CV F1 Mean': [results['cv_mean'] for results in model_results.values()],\n",
    "    'CV F1 Std': [results['cv_std'] for results in model_results.values()],\n",
    "    'Test Accuracy': [results['accuracy'] for results in model_results.values()],\n",
    "    'Test Precision': [results['precision'] for results in model_results.values()],\n",
    "    'Test Recall': [results['recall'] for results in model_results.values()],\n",
    "    'Test F1': [results['f1'] for results in model_results.values()]\n",
    "})\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(comparison_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "metrics = ['Test Accuracy', 'Test Precision', 'Test Recall', 'Test F1']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    comparison_df.plot(x='Model', y=metric, kind='bar', ax=axes[i], \n",
    "                      color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "    axes[i].set_title(f'{metric} Comparison')\n",
    "    axes[i].set_ylabel(metric)\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].legend().remove()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, (name, results) in enumerate(model_results.items()):\n",
    "    cm = confusion_matrix(y_test, results['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['FAKE', 'REAL'], yticklabels=['FAKE', 'REAL'], ax=axes[i])\n",
    "    axes[i].set_title(f'{name} - Confusion Matrix')\n",
    "    axes[i].set_ylabel('True Label')\n",
    "    axes[i].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curves\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "for name, results in model_results.items():\n",
    "    fpr, tpr, _ = roc_curve(y_test, results['y_pred_proba'][:, 1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.plot(fpr, tpr, linewidth=2, \n",
    "             label=f'{name} (AUC = {roc_auc:.3f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random Classifier')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Importance {#feature-importance}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest (best performing model)\n",
    "best_model_name = comparison_df.loc[comparison_df['Test F1'].idxmax(), 'Model']\n",
    "best_model = model_results[best_model_name]['model']\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    # Get feature importance\n",
    "    importance = best_model.feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_columns,\n",
    "        'importance': importance\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_features = feature_importance_df.head(15)\n",
    "    sns.barplot(data=top_features, x='importance', y='feature', palette='viridis')\n",
    "    plt.title(f'Top 15 Feature Importance - {best_model_name}')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance_df.head(10))\n",
    "else:\n",
    "    print(f\"{best_model_name} does not support feature importance.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance heatmap (for MFCC features)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    mfcc_features = [col for col in feature_columns if col.startswith('mfcc')]\n",
    "    mfcc_importance = []\n",
    "    \n",
    "    for feature in mfcc_features:\n",
    "        idx = feature_columns.index(feature)\n",
    "        mfcc_importance.append(importance[idx])\n",
    "    \n",
    "    # Reshape for heatmap (4x5 grid for 20 MFCC features)\n",
    "    mfcc_matrix = np.array(mfcc_importance).reshape(4, 5)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(mfcc_matrix, annot=True, fmt='.4f', cmap='YlOrRd',\n",
    "                xticklabels=[f'MFCC{i+1}' for i in range(5)],\n",
    "                yticklabels=[f'Group {i+1}' for i in range(4)])\n",
    "    plt.title('MFCC Feature Importance Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Deployment {#model-deployment}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model and preprocessing components\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save model\n",
    "model_data = {\n",
    "    'model': best_model,\n",
    "    'model_type': best_model_name.lower().replace(' ', '_'),\n",
    "    'training_history': {\n",
    "        'cv_scores': model_results[best_model_name]['cv_scores'],\n",
    "        'cv_mean': model_results[best_model_name]['cv_mean'],\n",
    "        'cv_std': model_results[best_model_name]['cv_std'],\n",
    "        'val_metrics': {\n",
    "            'accuracy': model_results[best_model_name]['accuracy'],\n",
    "            'precision': model_results[best_model_name]['precision'],\n",
    "            'recall': model_results[best_model_name]['recall'],\n",
    "            'f1': model_results[best_model_name]['f1']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(model_data, '../models/best_model.pkl')\n",
    "print(\"Model saved to ../models/best_model.pkl\")\n",
    "\n",
    "# Save preprocessor\n",
    "preprocessor_data = {\n",
    "    'scaler': scaler,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_columns': feature_columns\n",
    "}\n",
    "\n",
    "joblib.dump(preprocessor_data, '../models/preprocessor.pkl')\n",
    "print(\"Preprocessor saved to ../models/preprocessor.pkl\")\n",
    "\n",
    "# Save processed dataset\n",
    "os.makedirs('../data', exist_ok=True)\n",
    "df.to_csv('../data/DATASET-balanced.csv', index=False)\n",
    "print(\"Dataset saved to ../data/DATASET-balanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "print(\"=\" * 50)\n",
    "print(\"MODEL TRAINING SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Dataset Size: {len(df)} samples\")\n",
    "print(f\"Features: {len(feature_columns)}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(f\"  Cross-Validation F1: {model_results[best_model_name]['cv_mean']:.4f} ± {model_results[best_model_name]['cv_std']:.4f}\")\n",
    "print(f\"  Test Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  Test Precision: {model_results[best_model_name]['precision']:.4f}\")\n",
    "print(f\"  Test Recall: {model_results[best_model_name]['recall']:.4f}\")\n",
    "print(f\"  Test F1 Score: {model_results[best_model_name]['f1']:.4f}\")\n",
    "print(\"\\nFiles Saved:\")\n",
    "print(\"  - ../models/best_model.pkl\")\n",
    "print(\"  - ../models/preprocessor.pkl\")\n",
    "print(\"  - ../data/DATASET-balanced.csv\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
